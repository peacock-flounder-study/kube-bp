# 11장

### 클라우드 네이티브 정책 엔진: OPA와 Gatekeeper

규정을 준수하는 리소스를 평가하려면 유연한 정책 엔진이 필요

**오픈 정책 에이전트 (Open Policy Agent, OPA)**

**OPA**는 클라우드 네이티브 에코시스템에서 인기를 얻고 있는 유연하고 가벼운 **오픈 소스 정책 엔진**. 

OPA의 도입으로 다양한 Kubernetes 거버넌스 도구의 구현이 나타남

**Gatekeeper 소개**

**Gatekeeper**는 커뮤니티가 힘을 모으고 있는 Kubernetes 정책 및 거버넌스 프로젝트 중 하나

- Gatekeeper는 **클러스터 정책 및 거버넌스를 위한 사용자 정의 가능한 오픈 소스 Kubernetes 어드미션 웹훅**.
- **OPA 제약 프레임워크**를 활용하여 사용자 정의 리소스 정의(CRD) 기반 정책을 시행.
- CRD를 사용하여 **정책 작성과 구현을 분리하는 통합된 Kubernetes 환경**을 구현 가능.
- Gatekeeper는 리소스 유효성 검사 및 감사 기능을 지원하며, 모든 Kubernetes 클러스터에서 구현할 수 있는 **이식성**이 큰 장점.
- Gatekeeper를 통해 규정을 준수하는 Kubernetes 리소스 사양만 클러스터에 커밋되도록 허용함으로써, 다른 거버넌스 도구와 동일한 사용자 경험(UX)을 제공.

**일반적인 모범 사례 정책 예시**: 

- 신뢰할 수 있는 컨테이너 레지스트리의 컨테이너만 허용,
- 모든 컨테이너에 리소스 제한 지정,
- 서비스를 인터넷에 공개적으로 노출하지 않음 등

**게이트키퍼 용어 및 구성**

Gatekeeper는 OPA 제약 프레임워크를 사용하며 세 가지 주요 용어를 도입

- **A. 제약 조건 (Constraint)**
    - Kubernetes 리소스 사양의 특정 필드와 값에 적용하는 제한을 의미. ⇒ 실제로 정책을 길게 표현한 것
    - 제약 조건이 정의되면 효과적으로 **허용하지 않겠다**고 명시하는 것
    - Gatekeeper는 원하지 않는 필드와 값만 거부하며, 제약 조건 없이 리소스가 **암시적으로 허용**되는 방식을 취함. ⇒ 계속 변경되는 Kubernetes 리소스 사양에 매우 적합
    - **논리적 AND:**
        - 여러 정책이 동일한 필드의 유효성을 검사하는 경우, 한 정책이 위반하면 전체 요청이 거부됨. 이를 논리적 OR로 만들 수 있는 방법 X

- **B. Rego**
    - **OPA 네이티브 쿼리 언어**
    - Rego 쿼리
        - OPA에 저장된 데이터에 대한 assertion이며, Gatekeeper는 Rego를 **제약 조건 템플릿**에 저장

- **C. 제약 조건 템플릿 (Constraint Template)**
    - 사용자 지정 리소스 정의(CRD) 형식
        - CRD를 통해 정책을 **공유하거나 재사용**할 수 있도록 템플릿화하는 수단을 제공
    - 재사용할 수 있도록 매개변수를 입력하고 매개변수화된 Target rego로 구성
    - 구성 요소
        - **Kubernetes 필수 CRD 메타데이터**
            - 정책의 목적을 쉽게 파악할 수 있는 이름이 중요
        - **입력 매개변수 스키마**
            - validation field
            - 유효성 검사 필드로 표시되며 입력 매개변수와 유형 정의
        - **정책 정의**
            - `target` 필드로 표시
            - 템플릿화된 Rego 포함으로 이루어져 있음
    - 규칙이 일치하면 제약 조건이 위반된 것

- **제약 조건 정의**
    - 제약 조건은 정의된 **제약 조건 템플릿에 매핑**.
        - 예를 들어, 템플릿 이름이 `K8sAllowedRepos`인 경우 제약 조건의 `kind`도 `K8sAllowedRepos`와 일치해야 함
    - spec
        - `match` 필드: 정책의 의도 범위를 정의
            - (예: 특정 `apiGroups`, `kinds`, `namespaces` 일치).
        - `parameters` 필드:
            - 정책의 의도를 정의하며, 제약 조건 템플릿 스키마의 유형과 일치해야 함
                - (예: `openpolicyagent/`로 시작하는 컨테이너 이미지만 허용).
                - 제약 조건 작동 로직:
                    - 라벨 셀렉터 사용 가능
                    - 특정 종류 및 네임스페이스에서만 제한 가능
                    - 오류를 조기에 감지할 수 있는 스키마 유효성 검사 등을 포함

- **데이터 동기화 및 캐싱**
    
    특정 상황(예: "인그레스 호스트 이름이 겹치지 않아야 함")에서는 현재 리소스를 클러스터에 있는 다른 리소스와 비교해야 할 수 있음
    
    - 규칙 평가를 수행하기 위해 **OPA는 다른 모든 인그레스 리소스를 캐시에 보유**해야 함
    - Gatekeeper는 `config` 리소스를 사용하여 OPA에 캐시되는 데이터를 관리하며, 이는 감사 기능에도 사용됨
    - `Config` 리소스의 `sync` 필드
        - 어떤 종류와 버전의 리소스를 캐시할지 정의
            - 예: v1 서비스, 파드, 네임스페이스
            - **Kubernetes Secrets 같이 잠재적으로 민감한 데이터에 대한 동기화 및 적용은 OPA가 이 데이터를 캐시에 보관하기 때문에 잠재적인 공격 벡터에 노출될 수 있으므로 권장 X**

- 강제 조치 및 감사 (Enforcement Actions and Auditing)
    - Gatekeeper는 정의된 정책을 위반하는 리소스에 대해 클러스터 사용자에게 **실시간 피드백**을 제공.

**Enforcement Actions**

- `enforcementAction` 필드
    - 제약 조건에는 정책 위반 시의 동작을 정의
    - 기본값은 `deny`
    
    | EnforcementAction 값 | 동작 설명 |
    | --- | --- |
    | **`deny`** (기본값) | 정책을 위반하는 리소스 생성이 차단되며, 오류 메시지가 감사에 기록되고 사용자에게 다시 전송 |
    | **`warn`** | 리소스가 생성되지만, 경고 메시지가 감사에 기록되고 사용자에게 다시 전송 |
    | **`dryrun`** | 리소스가 생성되며, 정책을 위반하는 리소스가 감사 로그에만 표시 |

**감사 (Auditing)**

- Gatekeeper는 구성된 모든 정책에 대해 리소스를 주기적으로 평가하고 **감사 로그**를 제공하여 정책에 따라 잘못 구성된 리소스를 감지하고 수정할 수 있게 함
- 감사 결과는 제약 조건의 **`status` 필드**에 저장되므로 `kubectl`을 사용하여 쉽게 찾을 수 있음
    - `status` 필드
        - `auditTimestamp` : 마지막으로 감사가 실행된 시간
        - `violations` : 위반 리소스 목록
        - `totalViolations` : 총 위반 수


**모범 사례**

클러스터에 정책 및 거버넌스를 구현할 때 다음 사항을 고려

1. **적용 범위 엄격하게 지정:**
    1. 제약 조건을 적용하려는 리소스에 가능한 한 엄격하게 범위를 지정
    2. kind, 네임스페이스, 레이블 selector 사용
2. **가장 낮은 핸드오프 지점 선택**
    1. 파드의 특정 필드를 적용할 때, 디플로이먼트나 레플리카셋보다는 **런타임 전 가장 낮은 핸드오프 지점인 파드**를 선택하는 것이 가장 좋음
3. **단계적 롤아웃:**
    1. 이미 배포된 리소스가 있는 클러스터에서는 `enforcementAction`을 `deny`로 설정하기 전에 **`warn` 및 `dryrun`과 함께 감사를 활용**하여 정책을 위반하는 리소스를 식별하고 수정해야 함
4. **선언적 접근 방식 선호:** 
    1. 변경 정책을 사용하지 말고 GitOps를 포함한 다른 **선언적 접근 방식**을 고려해야 함

----

# 12장

### Why Multiple Clusters?

Kubernetes는 많은 워크로드를 단일 클러스터로 통합하기 위해 설계되었지만, 다양한 시나리오에서 둘 이상의 클러스터가 필요

**A. 폭발 반경 (Blast Radius) 및 시스템 손상 제한**

- 멀티클러스터 아키텍처를 설계하는 주된 이유 중 하나는 **폭발 반경**에 대한 우려 때문
- 마이크로서비스 아키텍처에서 circuit breaker, retry 등을 사용하여 손상 범위를 제한하듯이, 인프라 계층에서도 동일하게 설계해야 함
- 여러 클러스터를 사용하면 소프트웨어 문제로 인한 **연쇄적인 장애**의 영향을 방지 가능
    - 예를 들어, 하나의 클러스터에서 플랫폼 문제가 발생하여 500개의 애플리케이션에 영향을 미치는 대신, 5개의 클러스터를 사용하면 20%의 애플리케이션에만 영향을 미침.
        - 단점: 관리해야 할 클러스터 수가 늘어나고 통합 비율이 낮아짐

**B. 규정 준수 (Compliance)**

- PCI, HIPAA 등과 같은 **규제가 심한 워크로드**는 일반 목적 워크로드와 분리하면 관리가 더 쉬움.
- 이러한 워크로드는 보안 강화, 비공유 구성 요소, 전용 워크로드 요구 사항과 관련하여 특정 요구 사항이 있을 수 있으며, 클러스터를 분리하는 것이 특수한 방식으로 처리하는 것보다 쉬움

**C. 보안 및 규모 (Security and Scale)**

- **대규모 Kubernetes 클러스터의 보안**은 관리가 어려워질 수 있음.
    - 팀이 늘어나고 보안 요구 사항이 다를 때 대규모 멀티테넌트 클러스터에서 이를 충족하기 어려움
- 단일 클러스터에서 RBAC, 네트워크 정책, 파드 보안 정책을 대규모로 관리하는 것은 복잡하며, 작은 변경도 실수로 다른 사용자에게 보안 위험을 노출 가능
- 여러 클러스터를 사용하면 잘못된 구성으로 인한 **보안 영향 범위를 제한**할 수 있음

**D. 지역적 분산 및 특수 워크로드**

- **지역별 엔드포인트**에서 트래픽을 처리해야 하는 워크로드를 실행하는 경우
    - 여러 클러스터를 기반으로 설계해야 함.
    - 전 세계에 분산된 애플리케이션의 경우 여러 클러스터 실행이 필수적
- **특수한 워크로드**
    - 예: 고성능 컴퓨팅(HPC), 머신 러닝(ML)들은 특정 하드웨어와 고유한 성능 프로필을 요구하며, 이들을 전용 클러스터로 분리하는 것을 고려해야 함
- **하드 멀티테넌시**가 필요한 경우
    - 네임스페이스와 같은 소프트 멀티 테넌시도 있지만, 아래의 경우와 같은 클러스터 내부에서 해로운 워크로드들은 하드 멀티 테넌시가 필요
    - 예: 클라우드 제공업체, SaaS 호스팅, 신뢰할 수 없는 워크로드 호스팅 등과 같은 워크로드들은 전용 클러스터에 배포해야 함

### 2. 멀티클러스터 설계 문제 (Design Challenges)

멀티클러스터 설계를 선택할 때 아키텍처를 복잡하게 만들 수 있는 몇 가지 일반적인 문제가 발생할 수 있음

**데이터 복제 및 일관성**

- 여러 지역/클러스터에 걸쳐 워크로드를 배포할 때 핵심 요소.
- 복제 전략을 개발하고, 애플리케이션이 지역 간 지연 시간 또는 최종적인 일관성을 처리할 수 있도록 설계해야함

**서비스 디스커버리**

- 각 Kubernetes 클러스터는 자체 서비스 검색 레지스트리를 배포하며, 이는 여러 클러스터 간에 동기화되지 않아 식별이 복잡해짐.
- HashiCorp의 Consul, Istio, Linkerd, Cilium과 같은 도구가 클러스터 간 서비스 디스커버리 확장에 활용될 수 있음
- HashiCorp - Consul의 경우
    - 멀티 클러스터 서비스와 쿠버네티스 외부 서비스까지 동기화 가능

**네트워크 라우팅**

- 클러스터 간 송신 트래픽 라우팅이 복잡해짐.
- 클러스터로의 인그레스는 인그레스 리소스를 사용하는 멀티클러스터 토폴로지를 지원하지 않으므로 1:1 매핑으로 구현됨.
- 긴밀하게 연결된 종속성이 있는 애플리케이션은 지연 시간과 복잡성을 제거하기 위해 동일한 클러스터 내에서 실행하는 것이 좋음

**운영 관리**

- 관리할 클러스터가 많아지므로 운영 부담을 줄이기 위해 올바른 자동화 관행을 마련하는 것이 중요.
    - 테라폼과 같은 IaC도 하나의 방법
    - 하지만 인프라 배포와 모니터링, 로깅, 보안 등의 추가 기능(애드온)들의 관리를 고려해야 함.
- 특히 클러스터 전반에서 보안 정책, RBAC, 네트워크 정책을 일관되게 유지 관리 필요.

**지속적인 배포 (CD)**

- 단일 API 엔드포인트가 아닌 여러 Kubernetes API 엔드포인트를 처리해야 하므로 애플리케이션 배포에 문제가 발생할 수 있음.
- CD 전략은 여러 지역 또는 클러스터 간의 롤아웃을 처리할 수 있어야 합니다.

### 배포 및 관리 패턴

- 여러 클러스터를 성공적으로 관리하려면 자동화가 핵심이며, 재현 가능한 방식으로 클러스터를 배포할 수 있는 도구를 사용해야 함

**A. 인프라 자동화 및 IaC**

- **Terraform**과 같은 IaC(Infrastructure as Code) 도구를 사용
    - 여러 클러스터에 걸쳐 일관된 상태를 배포하고 관리할 수 있으며, 이는 반복성을 위해 중요
- **Cluster API**
    - 공통 API를 통해 클러스터 수준 구성을 선언적으로 제공하여 클러스터 자동화와 관련된 도구를 쉽게 구축할 수 있도록 지원하는 초기 단계 프로젝트

**B. Kubernetes Operator 패턴**

- **Kubernetes Operator**은 소프트웨어로서의 인프라(IaS) 개념의 구현
    - 애플리케이션 및 서비스 배포를 클러스터 수준에서 추상화
- 커스텀 리소스 정의(CRD)와 커스텀 컨트롤러 기반
    - CRD는 사용자 정의 API를 기반으로 Kubernetes API를 확장
    - 커스텀 컨트롤러는 이벤트를 감지하고 자체 논리를 구축하여 선언적 상태를 유지
- 오퍼레이터 패턴은 복잡한 운영 작업을 자동화하여 멀티클러스터의 운영 부담을 줄임
- 예시: `prometheus-operator`
    - Prometheus 배포와 관련된 모든 핵심 세부 사항
        - 몇 개의 오브젝트(Prometheus, ServiceMonitor 등)만으로 지정할 수 있게 해줌

**C. GitOps 접근 방식**

- GitOps는 Git 리포지토리가 진실의 소스(source of truth)가 되며 클러스터가 구성된 리포지토리에 자동으로 동기화되도록 함
    - **멀티클러스터를 일관성 있게 유지**하고 여러 클러스터에 걸친 구성 편차를 방지할 수 있음
    - 애플리케이션 제공과 운영 모두에 적용될 수 있으며, 클러스터와 운영 도구 관리에 활용
- GitOps 도구
    - **Weaveworks의 Flux**와 **Intuit의 Argo CD**
        - Flux를 사용하면 GitHub 리포지토리 상태를 Kubernetes 클러스터와 쉽게 동기화하여 'Snowflake 클러스터' 상황을 피하고 여러 운영 도구를 쉽게 관리 가능

### 멀티클러스터 관리 도구 및 페더레이션

**A. 멀티클러스터 관리 도구**

- **기본 유틸리티**
    - 여러 클러스터를 관리할 때 서로 다른 컨텍스트를 설정해야 하는 `kubectl`의 혼란을 해소하기 위해 **`kubectx`**와 **`kubens`**를 설치하여 컨텍스트 및 네임스페이스 간에 쉽게 변경할 수 있음
- **Rancher:**
    - 중앙에서 관리되는 UI를 통해 온프레미스, 클라우드, 호스팅된 Kubernetes 설정을 모니터링, 관리, 백업 및 복원 가능
    - 멀티 클러스터에 배포된 어플리케이션 제어 관련 각종 운영 툴 지원
- **OCM (Open Cluster Management):**
    - 멀티클러스터 및 멀티클라우드 시나리오에 중점을 둔 커뮤니티 프로젝트로, 클러스터 등록, 워크로드 배포, 정책 및 동적 워크로드 배치를 제공
- **Gardener:** 서비스형 Kubernetes(KaaS) 제품을 구축하는 사용자를 대상으로 함

--------------

# 13장

### Kubernetes로 외부 서비스 가져오기 (클러스터가 소비자가 됨)

가장 일반적인 통합 패턴

**→ Kubernetes 서비스가 Kubernetes 클러스터 외부에 존재하는 서비스를 소비하는 패턴**

A. 네트워킹 및 서비스 디스커버리의 초기 과제

1. **네트워킹 설정:** 
    1. 파드와 온프레미스 리소스 간에 네트워크 연결을 설정하는 것이 첫 번째 과제. 
    2. Cloud 기반 provider는 클러스터를 사용자 제공 가상 네트워크(VNET)에 배포하고, 이를 온프레미스 네트워크와 **피어링**할 수 있게 함
2. **서비스 디스커버리:** 
    1. 네트워크 연결이 설정된 후, 외부 서비스를 **Kubernetes 서비스처럼 보이게** 만드는 것. 
    2. Kubernetes에서 서비스 디스커버리는 DNS 조회를 통해 이루어지므로, 외부 데이터베이스를 동일한 DNS에서 검색 가능하도록 만들어야 함.

**B. 안정적인 IP 주소를 위한 셀렉터리스 서비스 (Selectorless Service)**

외부 리소스에 **안정적인 IP 주소**가 있다고 가정할 때 사용

- **구현:** selector 없이 Kubernetes `Service`를 생성하면, 일치하는 파드가 없으므로 **로드 밸런싱이 수행되지 않음**
- **작동 방식:** 대신, 이 서비스를 프로그래밍하여 외부 리소스의 특정 IP 주소인 **엔드포인트를 갖도록** 할 수 있음
    - Kubernetes 파드가 서비스 이름(예: `your-database`)에 대한 조회를 수행
    - → 기본 제공 DNS 서버가 이를 외부 서비스의 IP 주소로 변환
    - **YAML 예시:**
        - `Service` 리소스와 함께, `Service` 이름과 일치하는 `Endpoints` 리소스를 정의
        - `subsets` 필드 아래에 `ip: 24.1.2.3`와 같은 주소를 포함하여 포트와 함께 구성
            
            ```yaml
            apiVersion: v1
            kind: Endpoints
            metadata:
              # 중요! 이 이름은 Service 이름과 일치해야 합니다
              name: my-external-database
            subsets:
              - addresses:
                  - ip: 24.1.2.3 # Kubernetes 클러스터에 추가하려는 외부 리소스의 특정 IP 주소 [1]
                ports:
                  - port: 3306 # 외부 서비스가 실제로 실행되는 포트
            ```
            
- **장점:** 개발자는 서비스가 클러스터 외부에서 구현되었다는 사실을 모를 수 있음

**C. 안정적인 DNS 이름을 위한 CNAME 기반 서비스**

외부 리소스에 안정적인 IP 주소가 없거나 (예: 동적인 Cloud VM IP 주소) 

단일 DNS 기반 부하 분산 디바이스 뒤에 여러 복제본이 있을 때 사용

- **구현:**
    - `CNAME` (Canonical Name) 레코드를 사용하여 특정 DNS 주소가 다른 **캐노니컬 DNS 이름**으로 확인되도록 함
    - CNAME : 특정 dns 주소가 변환되는 다른 dns 네임을 나타내는 alias
- **유형:**
    - `type: ExternalName`을 사용하여 서비스를 정의하고, `externalName` 필드에 외부 DNS 주소를 지정
        
        ```yaml
        kind: Service
        apiVersion: v1
        metadata:
          name: myco-database
        spec:
          type: ExternalName
          externalName: database.myco.com
        ```
        
- **작동 방식:**
    - 이 서비스를 조회하는 모든 파드는 재귀적으로 외부 서비스의 `externalName`으로 resolve됨.
- **주의 사항 (DNS 확인):**
    - 이 방법이 작동하려면 외부 리소스의 DNS 이름이 **Kubernetes DNS 서버에서 확인**할 수 있어야 해야함
    - 만약 외부 DNS가 회사 로컬 DNS 서버(내부 트래픽만 서비스)에 있다면,
        - 쿠버네티스 클러스터는 이 사내 DNS 서버에 쿼리를 어떻게 리졸빙할지 모름
        - Kubernetes 클러스터의 DNS 서버 구성 파일로 k8s ConfigMap을 조정하여 대체 DNS 확인자와 통신하도록 설정해야 함
- **장점:**
    - `CNAME` 서비스는 네임스페이스별로 정의되므로, 네임스페이스를 사용하여 동일한 서비스 네임을 다른 외부 서비스에 매핑 가능
        - `database`을 다른 외부 서비스(예: `canary` 또는 `production`)에 매핑할 수 있음

**D. 액티브 컨트롤러 기반 접근 방식 (동적 서비스 통합)**

통합하려는 외부 서비스에 안정적인 IP 주소나 DNS 이름이 없는 경우에 사용되며, 이는 더 복잡함…

- **내부 작동 이해:**
    - Kubernetes 서비스는 `Service` 리소스와 해당 서비스를 구성하는 IP 주소를 나타내는 **`Endpoints` 리소스**로 구성
    - 정상 작동 시 컨트롤러 관리자는 service selector를 기반으로 엔드포인트를 채움
- **구현:**
    - 셀렉터 없는 서비스를 생성하면 `Endpoints` 리소스가 채워지지 않으므로, 제어 루프(액티브 컨트롤러)를 제공해야 함
- **작동 방식:**
    - 이 제어 루프는 인프라를 동적으로 쿼리하여 외부 서비스의 IP 주소를 가져온 다음, 이 IP 주소로 서비스의 `Endpoints` 리소스를 직접 채워야함
- **효과:** 이 작업이 완료되면 Kubernetes의 메커니즘이 DNS 서버와 `kube-proxy`를 모두 인수하여 외부 서비스에 대한 트래픽 로드 밸런싱을 올바르게 프로그래밍