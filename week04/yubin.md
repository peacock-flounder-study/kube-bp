# 07. 글로벌 애플리케이션 분산과 스테이징

### 7.1 이미지 분산 배포
- 글로벌 서비스는 이미지 레지스트리에서 지리적 복제를 지원하는 것이 가장 이상적
- 클라우드 레지스트리를 사용하면 자동 복제가 지원되나, 기능이 없을 경우 이미지 풀 레이턴시 및 단일 장애점 고려해야 함.  
- 해결 방법은 리전별 전용 레지스트리 운영, 네트워크 경로 최적화, 지역 명칭 부여.  
- 보안·성능 측면에서 직접 구축보다는 클라우드 기반 레지스트리 권장.  

### 7.2 배포 파라미터화
- 리전별 환경 차이를 헬름 템플릿 파라미터로 추상화.  
- 공통 차트 + 리전별 values.yaml 조합 → 운영 효율성 ↑, 설정 불일치 ↓.  
- 파라미터화 수준은 조직 내 운영 표준과 일관성을 고려해야 함.  

### 7.3 글로벌 트래픽 로드 밸런싱
- 전 세계 사용자에게 서비스를 제공하려면 지리 근접성을 우선 고려해야 함. → 레이턴시 최소화.  
- 동시에 서버 중단·서비스 장애를 대비해 리전 간 **페일오버** 전략을 마련해야 함.  
- 설정 목표: 리전별 트래픽 균등 분산 + 장애 시 빠른 우회.  

- 방식
  - **DNS 기반 로드 밸런싱**  
    - 단일 호스트네임(e.g. `myapp.myco.com`) 사용.  
    - DNS 서버가 사용자의 위치·리전 가용성을 반영하여 매번 다른 IP 주소 반환.  
    - 장점: 단순, 폭넓은 지원. 단점: TTL 문제로 전환 지연 발생 가능.  
  - **멀티케스트 주로 동일한 ip주소를 인터넷의 여러 위치에 전파**  
    - 항상 동일한 IP 주소 반환.  
    - 실제 라우팅은 네트워크가 사용자와 가장 가까운 물리 서버로 연결.  
    - 장점: 전환 속도 빠름, 유저 경험 개선.  

- 글로벌 트래픽 관리는 **성능 + 안정성** 두 가지 축을 동시에 충족해야 함.  

### 7.4 안정적인 글로벌 롤아웃
- 글로벌 롤아웃은 단일 리전 배포보다 리스크가 크므로 단계적 접근 필요.  

1. **사전 롤아웃 검사**  
   - 실제 트래픽 없이 전체 스택을 풀 스케일로 배포 후 테스트.  
   - 성능 지표: 초당 요청 수, 요청 레이턴시, CPU·메모리·네트워크 사용량 필수 확인.  
   - 애플리케이션이 단순할 때 초기 도입이 유리함.  

2. **카나리 리전**  
   - 제한된 실제 트래픽을 수용하는 전용 리전 운영.  
   - 내부 사용자·테스트 팀을 대상으로 조기 피드백 확보.  
   - 롤아웃 실패 가능성을 대비한 경고·복구 체계 마련 필요.  
   > 글로벌은 아니지만 개인적으로 지금 하고 있는 것과 너무 비슷

3. **리전 타입 식별**  
   - 각 리전의 트래픽 패턴·사용자 특성 파악 후 롤아웃 순서 조정.  

4. **글로벌 롤아웃 전략**  
   - 첫 프로덕션 리전 배포 → 일정 시간 대기 → 다음 리전 확장.  
   - 대기 시간은 과거 장애 발생 평균 시간의 약 2배가 적절함.  

5. **문제 발생 시 대응**  
   - 체크리스트 기반 대응, 단계별 예상 결과 기록.  
   - 트래픽을 다른 리전으로 우회, 데이터 복구 훈련을 사전에 수행해야 함.  

> 모범 사례 생략

# 08. 리소스 관리

### 8.1 쿠버네티스 스케줄러
- Control Plane에 위치, 파드 배치를 결정하는 핵심 컴포넌트.  
- 클러스터 자원 제약조건 + 사용자가 지정한 조건 기반으로 스케줄링 결정.  

#### 프레디킷
- 노드가 파드 배치 가능 여부를 판단하는 1차 필터.  
- 조건 충족 여부에 따라 true/false 반환.  

#### 우선순위
- Predicate 통과 노드에 점수를 매겨 최적 노드를 선택.  
- 동일 점수 시 라운드 로빈 방식으로 분배.  

### 8.2 고급 스케줄링 기법
- 기본적으로 쿠버네티스는 리소스 균등 분배를 지향.  
- 필요 시 직접 정책 제어 가능.  

#### 파드 어피니티와 안티-어피니티
- 파드 간의 상대적인 배치 규칙을 적용할 수 있음.  
- 스케줄링 방식을 변경하거나 스케줄러의 배치 결정을 오버라이드하는 것.  
- 예: 안티-어피니티 규칙을 적용해 동일한 노드에 파드를 몰아넣지 않고, 여러 데이터센터 영역에 분산시킴.  
- 파드 레이블 key/value 쌍을 적절히 설정해 특정 노드에 배치(어피니티)하거나, 반대로 특정 노드에 배치되지 않도록(안티-어피니티) 제어 가능.  
- 예시 시나리오: Nginx Deployment의 레플리카 4개가 있을 때 PodAntiAffinity를 적용하면, 노드 한 곳이 장애 나더라도 다른 노드에 분산된 레플리카 덕분에 서비스 지속 가능.  

#### 노드 셀렉터, 테인트, 톨러레이션
- **노드 셀렉터**: 특정 label의 노드에만 배치.  
- **테인트**: 특정 워크로드 외 파드 배치를 차단.  
- **톨러레이션**: 해당 테인트 노드에 배치 허용.  
- 테인트 모드  
  - NoSchedule / PreferNoSchedule / NoExecute / NodeCondition.  
- 테인트 기반 축출(evection): 실행 중 파드를 다른 노드로 이동시킴.  

### 8.3 파드 리소스 관리

#### 리소스 요청
- CPU/메모리 요청값 → 노드가 충족하지 못하면 대기 상태.  
- `kubectl top`으로 가용 자원 확인.  

#### 리소스 리밋과 QoS
- CPU 초과 시 throttling, 메모리 초과 시 파드 종료/재시작.  
- QoS Class  
  - Guaranteed / Burstable / BestEffort.  

#### PodDisruptionBudgets
- 최소/최대 축출 허용 파드 수 정의 → 가용성 보장.  

#### 네임스페이스 기반 관리
- 네임스페이스 단위로 쿼터, RBAC, 네트워크 정책 적용.  
- 다중 테넌시 환경에서 클러스터 자원 논리적 분리.  
- `kube-system`, `default`는 경합 위험으로 주의하기  

#### 리소스 쿼터 / 리밋레인지
- Quota: 네임스페이스 단위 자원 제한.  
- LimitRange: 요청/리밋 미설정 파드에 디폴트 값 부여.  

### 클러스터 스케일링

#### 수동 / 자동
- 오토스케일러: 파드 대기 상태 → 노드 수 확장.  
- 불필요 시 축소, 단 PDB 기반 안전 축출 권장.  

#### HPA
- Deployment 메트릭 기반 자동 수평 확장.  
- 동기화 주기, 업스케일/다운스케일 지연 시간 설정 가능.  

#### 커스텀 메트릭 HPA
- 외부 메트릭 API 기반 확장 (예: 메시지 큐 길이, 외부 LB 지표).  

#### VPA
- CPU·메모리 요청값 자동 조정.  
- scale-out 불가 워크로드(MySQL 등)에 유용.  
- 구성: Recommender / Updater / Admission Plugin.  

### 리소스 관리 모범 사례
- 파드 안티-어피니티로 워크로드 분산, 고가용성 확보.  
- 특수 노드(Taint) + 필요 워크로드(Toleration)로 전용 자원 운영.  
- 노드 풀로 워크로드 특성 분리, 비용·성능 최적화.  
- 모든 파드에 CPU/메모리 limit 설정하기.  
- 리소스 쿼터·리밋레인지로 팀 간 공평한 리소스 할당.  
- 변동성 높은 워크로드 → HPA 사용.  
- VPA는 scale-out 불가 워크로드에만 신중히 적용.  